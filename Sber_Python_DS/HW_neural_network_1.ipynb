{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2307ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.misc import derivative \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5782af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.linspace(2, 20, 100)\n",
    "a2 = np.linspace(7, 40, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8240d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, t2 = torch.from_numpy(a1.reshape(10,10)), torch.from_numpy(a2.reshape(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f4464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.cat((t1.unsqueeze(0), t2.unsqueeze(0)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543e1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(10, 6)\n",
    "        self.act = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(6, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        if not self.training:\n",
    "            x = nn.Softmax()(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d8f0b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4590,  0.2191, -0.7105, -0.5532],\n",
       "         [ 0.8409,  0.1559, -1.1488, -0.8073],\n",
       "         [ 1.2228,  0.0927, -1.5870, -1.0613],\n",
       "         [ 1.6048,  0.0295, -2.0253, -1.3153],\n",
       "         [ 1.9940, -0.0580, -2.4415, -1.5860],\n",
       "         [ 2.3837, -0.1473, -2.8561, -1.8579],\n",
       "         [ 2.7734, -0.2366, -3.2708, -2.1297],\n",
       "         [ 3.1631, -0.3259, -3.6854, -2.4016],\n",
       "         [ 3.5529, -0.4152, -4.1000, -2.6734],\n",
       "         [ 3.9426, -0.5045, -4.5146, -2.9453]],\n",
       "\n",
       "        [[ 1.5043, -0.0634, -1.9354, -1.3272],\n",
       "         [ 2.2068, -0.1870, -2.7319, -1.7982],\n",
       "         [ 2.9213, -0.3507, -3.4920, -2.2966],\n",
       "         [ 3.6358, -0.5144, -4.2522, -2.7950],\n",
       "         [ 4.3503, -0.6781, -5.0123, -3.2934],\n",
       "         [ 5.0647, -0.8418, -5.7725, -3.7919],\n",
       "         [ 5.7792, -1.0055, -6.5326, -4.2903],\n",
       "         [ 6.4937, -1.1692, -7.2928, -4.7887],\n",
       "         [ 7.2082, -1.3329, -8.0529, -5.2871],\n",
       "         [ 7.9227, -1.4966, -8.8131, -5.7855]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Fmodel()\n",
    "model.forward(batch.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6ffdb",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "* Realize Adagrad optimizer\n",
    "* accumulated += gradient^2\n",
    "* adapt_lr = lr / sqrt(accumulated)\n",
    "* w = w - adapt_lr*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a825173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    def __init__(self, model: nn.Linear, lr=0.001):\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "\n",
    "        self.vel_w = np.zeros_like(model.w)\n",
    "        self.vel_b = np.zeros_like(model.b)\n",
    "        self.accum_w += self.model.d_w**2\n",
    "        self.accum_b += self.model.d_b**2\n",
    "        self.adapt_lr_w = lr / np.sqrt(self.accum_w)\n",
    "        self.adapt_lr_b = lr / np.sqrt(self.accum_b)\n",
    "\n",
    "    def step(self):\n",
    "        self.vel_w = self.m * self.vel_w - self.adapt_lr_w * self.model.d_w\n",
    "        self.vel_b = self.m * self.vel_b - self.adapt_lr_b * self.model.d_b\n",
    "\n",
    "        self.model.w += self.vel_w\n",
    "        self.model.b += self.vel_b\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.model.d_w = np.zeros_like(self.model.d_w)\n",
    "        self.model.d_b = np.zeros_like(self.model.d_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce760be8",
   "metadata": {},
   "source": [
    "**Task 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cffe45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax2 + bx + c = 0\n",
    "\n",
    "function = lambda x: x**2-9\n",
    "\n",
    "\n",
    "def step(x_new, x_prev, iterations=100):\n",
    "        \n",
    "    x_list, y_list = [x_new], [function(x_new)]\n",
    "    while iterations > 0 and function(x_new) != 0:\n",
    "        \n",
    "        x_prev = x_new\n",
    "        x_new = x_prev - function(x_prev)/derivative(function, x_prev)\n",
    "        x_list.append(x_new)\n",
    "        y_list.append(function(x_new))\n",
    "        iterations -= 1\n",
    "    print(f\"step {len(x_list)}: {x_list[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6770f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6: -3.0\n"
     ]
    }
   ],
   "source": [
    "step(-4, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
